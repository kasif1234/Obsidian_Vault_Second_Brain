{
	"nodes":[
		{"id":"1d80811a6cf30c6a","type":"text","text":"2 REPOS: NL2SQL & Paraphrasing Project -> Understand these","x":-540,"y":-320,"width":250,"height":114,"color":"4"},
		{"id":"8f2fb780fcbe51df","type":"text","text":"Transcribe Shahmeers Original Video, [Inernalize]","x":-540,"y":-166,"width":250,"height":60,"color":"4"},
		{"id":"e0b75804f77b6411","type":"text","text":"Collect Papers given by Micheal","x":-540,"y":-69,"width":250,"height":60,"color":"4"},
		{"id":"e83143714745e90b","type":"text","text":"Use NotebookLM to Understand each Paper","x":-540,"y":34,"width":250,"height":60},
		{"id":"b43a58d2cd179136","type":"text","text":"Prepare a Table","x":-540,"y":140,"width":250,"height":60},
		{"id":"ff4999840b7ea9c7","type":"text","text":"NL2SQL - https://github.com/MoAlthaf/NL2SQL\nParaphrasing Project - https://github.com/MoAlthaf/ParaphrasingProject","x":-225,"y":-326,"width":565,"height":86},
		{"id":"0fd0f05000a0b855","type":"text","text":"https://drive.google.com/drive/folders/10DEdFZ484nGQAyAhAVI1J4deujRyaWqg?usp=sharing","x":-199,"y":-166,"width":539,"height":97},
		{"id":"19480ebf928fea81","type":"text","text":"QCRI","x":-275,"y":-480,"width":100,"height":53},
		{"id":"2ae17e8e8cc87dcb","type":"text","text":"1. Bolographic Review\n2. Literature Review\nAccess the citations and references within the given papers\n========================================================================","x":-67,"y":-9,"width":947,"height":256},
		{"id":"e128681afff49a0a","type":"text","text":"Shameer Vocie - You just keep going like this. Yeah.\n\nSay you have like these three LLMs. You give them question Q. Okay. Q.\n\nSo you give each one the input which is Q. Okay.\n\nMhm.\n\nThe output from these models is some SQL. Okay. So we'll call this let's say SQL 1, SQL 2, SQL 3, right?\n\nYou run the query, right? And then you get some answer, right?\n\nYou get a score. So you check if this answer, so answer one, we'll call this answer prime, which is we know the correct answer, right? So we check if answer one is equal to answer prime, answer two is equal to answer prime, answer three is equal to answer prime. Right?\n\nSo you're basically in a nutshell, all you're doing is checking if you can take this question, the original question, right? Not the paraphrased one. You give it to a set of LLMs. Can they solve it using text to SQL?\n\nOkay.\n\nOkay.\n\nSay now you end up with this. Okay. So this LLM one and three can solve it. Two couldn't.\n\nOkay.\n\nOkay.\n\nThen basically you take only these two models. So you take LLM3 and you take LLM1 and you give it and you have some paraphrasing mechanism which could be LLM or anything, right? So then you give it q prime. Okay.\n\nYou already verify that q prime is correct or you—\n\nNo no, this is how we're going to verify q prime. Q prime is just some paraphrase text. We don't actually know if it's correct right now or not.\n\nSo you take Q prime, you give it to these two LLMs again and you repeat the process and you make sure that any LLM that could—any text to SQL model that could solve Q should be able to solve Q prime.\n\nOkay.\n\nRight, that's the idea. So say LLM1 can still solve Q prime, right?\n\nSay this is the case, like can, can't, right? We say the paraphrasing is wrong.\n\nOkay.\n\nRight. So we basically need a tick tick to be able to say that paraphrasing is correct.\n\nI want the two to be able to do it.\n\nYeah. Any, any, any model that did this before should be able—any model that can solve Q should be able to solve Q prime.\n\nOkay.\n\nAnd you verify this with multiple models.\n\nBut these are not necessarily the—they are the same.\n\nSo you assume they are good in text to SQL.\n\nYeah.\n\nThey should be good to paraphrasing.\n\nYeah. For my—No, no, no. They're not. So again the paraphrasing is agnostic, right? This is also just text to SQL. The idea I have is that Q and Q prime should be similar enough that if a model can solve Q, it should be able to solve Q prime because they're paraphrase, right? They should be very close to each other.\n\nSame task.\n\nSame task, same table, same everything. And they should be close enough to do it, right?\n\nThis is a high level to just to check if the question is correct or not, right? Like the output should match the original ground truth here, right?\n\nOkay.\n\nThe second problem we'll have, which I think about more is that say you have two questions that are very close that meet this criteria. So you have two different questions. You've done the paraphrasing both of them. The original and the paraphrase one can be solved by the same LLMs, right? The problem is how do you know that they're different enough?\n\nRight? Like what if you have a question like, what were sales in 2020, right?\n\nAnd now you were like what was sales in year 2020, right? Like whether this is a correct paraphrasing or not is arbitrary, right? Like for some cases like this might be considered correct, but for a lot this would probably be not considered like correct paraphrasing, right?\n\nOkay.\n\nLike a correct paraphrasing. So for example, this is your original, right? This is your paraphrased one. And another way of writing the same question is report the sales amount for year.\n\nYou see it's not correct in the sense it's too close. I'm saying that there's no—like you know there's no literature—like there's no way for you to like, you could say that this is a correct paraphrasing and I could say this is wrong, right? But wrong because the semantic is correct but it's too close. You haven't actually paraphrased it or changed it enough for me to give, right?\n\nSo this we need to figure out a way to measure this, the difference. We need to measure that the paraphrased one is far enough from the original that we can consider it to be actually paraphrase.\n\nOkay.\n\nRight. Like say that we have some similarity score measure, right, between—so say we have some function. I don't know what this function is right now. So the function between in this case O and P1 is like, I don't know, 0.5, right? And the similarity between O and P2 is like 0.2, right?\n\nSo clearly like P2 is a bit further than O and P1 is a bit closer to O. And then we can set a threshold that basically like this function has to be less than I don't know X, you know, has to be less than 0.3 for it to be considered like a correct tracing.\n\nOkay.\n\nRight, what this S is, I have no idea.\n\nOkay.\n\nLike some similarity metric, right?\n\nOkay.\n\nSo if you can basically—if you meet this criteria and you meet this criteria, then you can guarantee that your paraphrased question adds variety, like is linguistically different but semantically the same.\n\nOkay.\n\nSo it's asking the same thing but it's linguistically different, right?\n\nThese are two very different problems. Like this is just correctness and this is linguistics.\n\nSo this is an area I'm not very familiar with, need to read up a bit more, think of—so globally the objective is to generate, to check paraphrasing of question on table.\n\nYes.\n\nRight.\n\nYes.\n\nAutomatically so we can scale up and train models with that data because or the ones you have to ask human to do it.\n\nSo say you have like these 15 questions, right? You have 15 questions per table. Say I want to generate like a 100 variations of each question per table. Yeah.\n\nRight. How do I know if a) those questions are not super similar to each other and b) if they're actually correct? Like what if I change the question? What if I change the answer? What if I'm asking a different thing now?\n\nYeah.\n\nRight. So that's like the paraphrasing part.\n\nAnd the sort of sister project to this which is basically like assume you don't have this. Yeah. Right. We assuming you have Q and you—Yeah. I mean A can be derived. Assume you have Q. What if you don't have Q? What if you want to generate Q from scratch? Basically you just have a table. Yeah. A SQL table and you want to generate some set of questions given some sort of constraints, right?\n\nYeah.\n\nHow do you do that? So that is more open. A lot of people have actually worked on this. But what will be good is like—but then you can check that if you manage to say okay I have this pipeline. You can have a generator question and pass it to the pipeline to see if—if I don't have an answer—you don't have the answer, right? That's a problem.\n\nSo imagine a modified version of this basically. Okay. So say you have some system that generates some question but you don't have the answer, right? And say you give it to like three LLMs to generate just the text to SQL.\n\nOkay.\n\nRight. You generate the SQL and you basically assume that if two or more or certain percent—like if two LLMs generate the same SQL with the same output—not the same SQL sorry—the same answer then that should be the ground truth.\n\nOkay.\n\nRight. Something like this. Like some automated-ish mechanism to do this, right?\n\nAnd then you can basically like—but but this is again—this goes into—this is actually not that easy of a problem. Like if you think about like even these 15 questions templates, right? They're not arbitrary. They're not just yeah you came up with like—they are structured. They have some logic. They have the idea that you want to test certain things in your question, right?\n\nYeah.\n\nSo say you think your model is weak on row filtering, like on choosing the right rows. Yeah. Right? Like where conditions in SQL. And you want questions that specifically address that, right?\n\nSo given an arbitrary input of constraints, right? I want my questions to look like this, look like this, etc., etc., etc. How do you guarantee that your question does that, right?\n\nAnd you can—I hate like using the word agentic—but like you can think of it as like this sort of agentic system, right? So you have a set of constraints, right? You give this to an LLM. LLM gives you a question. You check that question if it meets the constraints. If it doesn't meet the constraints you loop back.\n\nOkay.\n\nTo this process, right, until it meets all the constraints.\n\nOkay.\n\nThis is all done through—we have this—I remember the reviews we had these guys saying there is a benchmark standard benchmark for testing database system and there are list—\n\nYes yes.\n\nIs it some kind of things we could reuse?\n\nSo those are fixed, those are fixed. They're not actually—but there are many compared to what we—no it's actually a super small benchmark.\n\nI see.\n\nThe idea there is that they generate very complex queries. So they generate extremely complex SQL queries on a given database but they're only having less than 20 databases.\n\nIn the—\n\nBut we cannot—these are not template—we cannot push it to any other database. They're SQL, they're only SQL. There's no natural language.\n\nYeah they're SQL, but from SQL we can generate natural language.\n\nYeah, but—\n\nAnd then paraphrase and then come back. We could, we could, but those are—I don't think we should start. They're an extremely complex version of SQL, right?\n\nYeah yeah.\n\nThey have multiple joins, multiple nested queries, multiple where conditions. Like I think we can eventually get to that point. We should start with like basic like select this where this, sort this, that's it. Those are the three conditions, right?\n\nSo you could start with this. So like relatively open space. There's no need for very crazy solutions. Like in reality we're not really doing anything novel, right? We're just using existing systems in a framework.\n\nOkay.\n\nSo you're just putting the ideas together. These LLMs already exist. The text to SQL generators already exist. Running SQL already exists, right? This mapping the answer is just like equality, right? Or Google is doing some verification the ground truth. This is just—there's no development here. It's some more of an engineering problem. But if you can show it works, it is a novel contribution that people can then use. Like they just use your script off the shelf, right?\n\nOkay.\n\nClone your repo and use it.\n\nYeah.\n\nSame with this. I'm hoping that someone has already solved this problem, like this S—we don't have to make it to see whether two things are linguistically similar or not. I'm hoping someone already has made this, which is like an adversary kind of approach where you have one which is trained to diversify but if you diversify too much then the other one cannot recognize.\n\nYeah, the solution is what are these models called? They are called contrasted but there is the generator and the discriminator.\n\nYeah yeah, sort of thing.\n\nMaybe we could do that, but again I'm hoping we don't have to. Like we can train the models using things like this. But I'm hoping someone has had this problem before and they have already trained a model that does this for us. So we can just use this, right? So everything is just a plug and play like coding thing. But that's the idea.\n\nYeah. At first you should plug and play to make the pipeline works. Then improve each part, right? Again, you already have this once you're done with your implementation of this. You already have this starting point. You have the tables, you have the questions, you have the ground rules.\n\nYep.\n\nRight. And then you solve start with this.\n\nOkay.\n\nAnd the LLMs I can pick, right? Or whatever. I don't know, the three best, Llama, Gemma, and Qwen. The three most popular ones.\n\nOkay, pretty good.","x":-1280,"y":-615,"width":600,"height":649,"color":"2"},
		{"id":"03b0ebcd32c280e9","type":"text","text":"## Goal\n\nAutomatically validate paraphrased NL questions for text-to-SQL datasets so you can scale from a small set of seed questions (e.g., 15 per table) to many variations (e.g., 100 per question) without human labeling.\n\n---\n\n## Core Objects\n\n- **Q**: original natural-language question (has ground-truth answer)\n    \n- **Q′**: paraphrased version of Q (unknown correctness initially)\n    \n- **LLM₁…LLMₙ**: text-to-SQL models\n    \n- **SQLᵢ**: SQL produced by LLMᵢ\n    \n- **Aᵢ**: answer from executing SQLᵢ on the database\n    \n- **A***: known correct answer for Q (ground truth)\n    \n\n---\n\n## Problem 1: Paraphrase Correctness (Semantic Equivalence)\n\n### Idea\n\nIf a model can solve **Q**, it should also be able to solve **Q′** because they represent the same task, same table, same intent.\n\n### Procedure\n\n1. **Baseline solvability set**\n    \n    - Give **Q** to multiple text-to-SQL models.\n        \n    - Execute their SQL and compare outputs to **A***.\n        \n    - Keep only the models that solve **Q** correctly.\n        \n    - Call this set **S(Q)**.\n        \n2. **Paraphrase verification**\n    \n    - Give **Q′** to the same solving models in **S(Q)**.\n        \n    - Execute SQL and compare results to **A***.\n        \n    - **Accept Q′ as correct** if all (or a high fraction) of models in **S(Q)** still solve it.\n        \n    - **Reject Q′** if models that previously solved Q fail on Q′.\n        \n\n### Output\n\nA “tick” (pass/fail) for whether **Q′ is semantically correct**.\n\n---\n\n## Problem 2: Paraphrase Diversity (Linguistic Distance)\n\n### Motivation\n\nTwo questions can be semantically identical but too similar in wording, meaning the “paraphrase” adds no real linguistic variety.\n\n### Need\n\nA similarity function **S(O, P)** between:\n\n- **O = Q** (original)\n    \n- **P = Q′** (paraphrase)\n    \n\n### Conceptual Rule\n\n- If **similarity(Q, Q′)** is **too high**, the paraphrase is rejected as “not different enough.”\n    \n- If similarity is **below a threshold**, it is considered a meaningful paraphrase.\n    \n\n### Example\n\n- Similarity(Q, P1) = 0.5 (too close)\n    \n- Similarity(Q, P2) = 0.2 (more diverse)\n    \n- Threshold might be < 0.3 to accept linguistic diversity\n    \n\n### Key Point\n\nThis is a separate problem from correctness:\n\n- **Correctness** = semantic equivalence\n    \n- **Diversity** = wording distance\n    \n\n---\n\n## Scaling Use Case\n\nFor each table:\n\n- Start with ~15 seed questions.\n    \n- Generate ~100 paraphrases per question.\n    \n- Automatically filter them by:\n    \n    1. semantic correctness (Problem 1)\n        \n    2. linguistic diversity (Problem 2)\n        \n\n---\n\n## Sister Project: Question Generation From Scratch (No Ground Truth)\n\n### Setting\n\nYou have only a database/table, not a known correct answer.\n\n### Proposed Heuristic\n\n1. Generate question **Q** using some generator (LLM).\n    \n2. Ask multiple text-to-SQL models to produce SQL.\n    \n3. Execute the SQL and compare **answers**.\n    \n4. If **two or more models agree on the same answer**, treat that as “pseudo-ground truth.”\n    \n\n### Challenge\n\nHarder than paraphrasing because good questions are not arbitrary:\n\n- Questions must be structured to test specific capabilities (e.g., WHERE filtering, sorting, aggregation).\n    \n- You may need a constraint-driven loop:\n    \n    - generate → check constraints → regenerate until constraints satisfied\n        \n\n---\n\n## Implementation Philosophy\n\n- Start “plug-and-play”:\n    \n    - existing LLMs for text-to-SQL\n        \n    - existing SQL execution\n        \n    - equality match for answers\n        \n- Contribution is primarily engineering + evaluation:\n    \n    - build a practical pipeline that others can clone and use\n        \n- Improve components later:\n    \n    - better diversity metric\n        \n    - better verification thresholds\n        \n    - better model selection strategies","x":-2160,"y":-740,"width":820,"height":900,"color":"5"},
		{"id":"fc6fd8d473507533","type":"text","text":"DR Micehal (18/1/2026) - )We will kind of explore all the paper from 10 years ago up to today cited in most big conference because you may also find paper which have no value. They are just archive and nothing more. They might be an unknown workshop or seminar somewhere, not really strong. You will see, you will feel it when you see it not well written, etc.\n\nSo it’s mostly like this back and forth between citation who sits inside this paper, and this paper themselves, they have reference. So sometimes you can also look at the reference because then they have a related work, and when you read the related work it explain, it gives you a hint about oh maybe this reference is interesting to me, and it gives you the keywords to use because you don't know at the beginning which keyword to use to detect the paper with which doing what you do.\n\nOf course you can use NL to SQL. You can use paraphrasing. But maybe the architecture you are using has a specific name. Yeah. Maybe people already explore this spontaneous, that specific architecture. So in that case it becomes more difficult, but maybe people give names, and it happens sometimes they give the same names, sometimes they give different names, but you have to collect these names and do the search with these names, and progressively you will get the big picture, right.\n\nPerfect.\n\nYou don't need to read the paper in full when you do that. You don't need to read every single details, it would take hours. Just get the over— you just need to skim through the abstract. Does it go in the same direction? Okay.\n\nYou do it in several paths: by title, then you go by abstract, then you go by—okay, for this one, let’s look at the schema. Is there an illustration? Start with a simple theme. Is the schema doing something very different? Just put it on the side. Schema is similar to you, keep it because it can be helpful for your related work.\n\nYeah.\n\nIf not for your experiment, can be useful for your related work. So you see, this is like—no, no really magical method to do this part I think.\n\nYeah. Yeah. So I was in the impression that there's like this procedure, step-by-step procedure, but I guess after your explanation this is snowballing. So you start from one and you expand the search from that one then you backtrack.\n\nYeah. To see some—because the point is to find the keywords which represent what you do and there is no name for that. So you have to find the combination of keywords. But now with the new tools, if you describe your idea, instead of just putting a keyword you say, give me paper that is doing— that is comparing several LLM for different paraphrasing and using a database for comparing the quality, and you give that to—","x":-2230,"y":-1120,"width":960,"height":320,"color":"2"},
		{"id":"fdd7236ac2a6449c","x":-1175,"y":-1800,"width":950,"height":591,"color":"5","type":"text","text":"## Objective\n\nBuild a strong literature map by finding the most relevant, high quality papers (not just anything that exists).\n\n---\n\n## Key Principles\n\n- Not all papers are worth reading. Some are low quality or “archival” workshop style work.\n    \n- You learn to spot weak papers quickly from writing quality and relevance cues.\n    \n\n---\n\n## Main Method: Snowballing\n\nYou move back and forth between:\n\n- **Citations inside a paper** (who the paper cites)\n    \n- **Papers that cite that paper** (forward citations)\n    \n- **References and Related Work sections** (these often contain the best leads)\n    \n\nThis “backtracking and expanding” is how you grow from a few seed papers into the full landscape.\n\n---\n\n## The Real Goal: Discover the Right Keywords\n\n- At the beginning, you do not know the best search terms.\n    \n- The related work section helps you:\n    \n    - identify what your idea is “called” in the community\n        \n    - collect alternative names (same concept can have multiple names)\n        \n- You then search again using these discovered terms and keep refining.\n    \n\n---\n\n## Efficient Reading Strategy (Do not read full papers early)\n\nUse a staged filter:\n\n1. **Title scan**: quick relevance check\n    \n2. **Abstract skim**: same direction as your work or not\n    \n3. **Figures / schema / diagrams**: does the setup resemble your problem\n    \n    - if schema is very different, set aside\n        \n    - if schema is similar, keep (useful for experiments or at least related work)\n        \n\n---\n\n## Practical Note\n\nThere is no “magical step-by-step recipe.”  \nIt is an iterative process: start from a few papers, expand, refine keywords, repeat until you see the big picture.\n\n---\n\n## Using Modern Tools\n\nInstead of only keyword search, you can use natural language search prompts like:\n\n- “Find papers comparing multiple LLMs for paraphrasing and validating via database / text-to-SQL outputs.”\n    \n\nThis helps when the correct keyword label is not obvious yet."}
	],
	"edges":[
		{"id":"294bf28f6975b172","fromNode":"1d80811a6cf30c6a","fromSide":"bottom","toNode":"8f2fb780fcbe51df","toSide":"top"},
		{"id":"f1f5656ea0febd7c","fromNode":"8f2fb780fcbe51df","fromSide":"bottom","toNode":"e0b75804f77b6411","toSide":"top"},
		{"id":"fd35771bc46a0856","fromNode":"e0b75804f77b6411","fromSide":"bottom","toNode":"e83143714745e90b","toSide":"top"},
		{"id":"457f3dc1757057a0","fromNode":"e83143714745e90b","fromSide":"bottom","toNode":"b43a58d2cd179136","toSide":"top"},
		{"id":"e69eaa393993d5e5","fromNode":"1d80811a6cf30c6a","fromSide":"right","toNode":"ff4999840b7ea9c7","toSide":"left"},
		{"id":"67176c269b4bca90","fromNode":"8f2fb780fcbe51df","fromSide":"left","toNode":"e128681afff49a0a","toSide":"right"},
		{"id":"daafb3345c453dba","fromNode":"e0b75804f77b6411","fromSide":"right","toNode":"0fd0f05000a0b855","toSide":"left"},
		{"id":"2b49e7a33f00460c","fromNode":"e83143714745e90b","fromSide":"right","toNode":"2ae17e8e8cc87dcb","toSide":"left"},
		{"id":"accabcfad5c67fb2","fromNode":"e128681afff49a0a","fromSide":"left","toNode":"03b0ebcd32c280e9","toSide":"right"},
		{"id":"6d3a0fc5bc079284","fromNode":"03b0ebcd32c280e9","fromSide":"top","toNode":"fc6fd8d473507533","toSide":"bottom"},
		{"id":"d1fbf2f8f5983501","fromNode":"fc6fd8d473507533","fromSide":"top","toNode":"fdd7236ac2a6449c","toSide":"bottom"}
	]
}